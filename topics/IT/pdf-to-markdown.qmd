---
title: "pdf-to-markdown"
execute:
  echo: false
format: html
editor: 
  markdown: 
    wrap: 72
---

### PDF to Markdown

[Source
Article](https://medium.com/data-science-collective/convert-pdfs-to-markdown-using-local-llms-c5232f3b50fc)

Convert any PDF to clean, structured Markdown using a local LLM (Gemma 3
via Ollama) — no cloud APIs, no privacy worries.

The idea is simple:

1.  Turn each PDF page into an image.
2.  Send those images to a local LLM using Ollama, specifically
    gemma3:12b (gemma3:4b works too).
3.  Ask the model to extract the readable content in Markdown format.
4.  Save the result as a .md file you can actually use.

#### Install newest uv in windows:

```         
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
```

#### Create an uv environment

```         
uv init pdftomd cd pdftomd
```

#### Add to the environment the required packages:

uv pip install pymupdf pillow ollama Make sure you have Ollama installed
(otherwise follow instructions in https://ollama.com/download)and
running locally, and that gemma3 is pulled:

```         
ollama run gemma3:12b
```

#### Alternatively if you don't want to use too much ressources:

```         
ollama run gemma3:4b
```

To run the code in a Python shell which uses the defined uv environment,
you do:

```         
uv run python 
```

from inside the pdftomd folder:

```         
uv run myscript.py 
```

Let’s start the code session with:

import fitz \# PyMuPDF for PDFs import ollama import io from PIL import
Image We import the essentials. PyMuPDF (via fitz) is great for page
rendering, and Pillow helps convert raw data into proper PNGs.

Step 1: Convert PDF Pages to Images

def convert_pdf_to_images(pdf_path): images = \[\] doc =
fitz.open(pdf_path) \# Open the PDF for page_num in range(len(doc)): pix
= doc\[page_num\].get_pixmap() \# Render page to pixel map img =
Image.frombytes("RGB", \[pix.width, pix.height\], pix.samples) \#
Convert to PIL image img_buffer = io.BytesIO() img.save(img_buffer,
format="PNG") \# Save as in-memory PNG
images.append(img_buffer.getvalue()) \# Raw PNG bytes return images This
function reads the PDF, converts each page into a high-res image, and
stores them in memory as raw PNG bytes — perfect for sending to an LLM
that accepts images.

Why use raw bytes? Because Ollama supports them directly. No need to
write files to disk, which is faster and cleaner.

Step 2: Ask the LLM to Extract Text

prompt = "Extract all readable text from these images and format it as
structured Markdown." def query_gemma3_with_images(image_bytes_list,
model="gemma3:12b", prompt=prompt): response = ollama.chat( model=model,
messages=\[{ "role": "user", "content": prompt, "images":
image_bytes_list }\] ) return response\["message"\]\["content"\] This is
where the magic happens. You’re sending the image data to your local
Gemma model and asking it to do the heavy lifting.

Bonus: Since everything runs locally, your data never leaves your
machine. Great for sensitive documents.

Step 3: Putting It All Together

pdf_path = "mypdf.pdf" \# Replace with your PDF file images =
convert_pdf_to_images(pdf_path)

if images: print(f"Converted {len(images)} pages to images.")

```         
extracted_text = query_gemma3_with_images(images)

with open("output.md", "w", encoding="utf-8") as md_file:
    md_file.write(extracted_text)
print("\nMarkdown Conversion Complete! Check `output.md`.")
```

else: print("No images found in the PDF.") This final section ties it
all together.

We load your PDF. Convert it to images. Feed it to the model. Save the
result to output.md. All in one go.

What You Gain ✅ Markdown-ready output, perfect for LLM pipelines,
knowledge bases, or just human readability. ✅ Scanned PDF
compatibility, thanks to the image-based approach. ✅ Private by
default, since all inference runs locally. ✅ Elegant simplicity — no
bloated OCR pipelines or brittle PDF parsers. Use Cases to Inspire You

Turn old scanned textbooks into Markdown for fine-tuning models Build an
offline document QA system using local embeddings Feed converted
documents into a retrieval-augmented chatbot Summarize meeting notes,
scientific papers, or financial reports Create a Markdown knowledge base
from PDFs, Word files, etc. Final Thoughts We often chase complexity
because we think that’s where the power lies. But sometimes, it’s about
removing friction and making things just click.

This workflow is fast, local, and smart. It gives you Markdown from
virtually any PDF with minimal effort, and it runs in a few seconds with
zero cloud dependencies.

The full code:

import fitz \# PyMuPDF for PDFs import ollama import io from PIL import
Image

def convert_pdf_to_images(pdf_path): images = \[\] doc =
fitz.open(pdf_path) \# Open the PDF for page_num in range(len(doc)): pix
= doc\[page_num\].get_pixmap() \# Render page to pixel map img =
Image.frombytes("RGB", \[pix.width, pix.height\], pix.samples) \#
Convert to PIL image img_buffer = io.BytesIO() img.save(img_buffer,
format="PNG") \# Save as in-memory PNG
images.append(img_buffer.getvalue()) \# Raw PNG bytes return images

prompt = "Extract all readable text from these images and format it as
structured Markdown." def query_gemma3_with_images(image_bytes_list,
model="gemma3:12b", prompt=prompt): response = ollama.chat( model=model,
messages=\[{ "role": "user", "content": prompt, "images":
image_bytes_list }\] ) return response\["message"\]\["content"\]

if **name** == '**main**':

```         
pdf_path = "mypdf.pdf"  # Replace with your PDF file
images = convert_pdf_to_images(pdf_path)

if images:
    print(f"Converted {len(images)} pages to images.")

    extracted_text = query_gemma3_with_images(images)

    with open("output.md", "w", encoding="utf-8") as md_file:
        md_file.write(extracted_text)
    print("\nMarkdown Conversion Complete! Check `output.md`.")
else:
    print("No images found in the PDF.")
```

Save this code under nano pdftomd.py and you can run this with:

uv run pdftomd.py You can extract text from a single image with:

import ollama import base64

def image_to_text(image_path, model="gemma3:12b", prompt="Extract text
from this image"): with(open(image_path, "rb") as f: image_data =
f.read() response = ollama.chat(model=model, messages=\[{"role": "user",
"content": prompt, "images": \[image_data\]}\]) return
response\["message"\]\["content"\] And you can improve your results with
a better prompt:

prompt = "Extract all readable text and text chunks from this image" +\
" and format it as structured Markdown." +\
" Look in the entire image always and try to retrieve all text!" By
changing the prompt and asking for another format (e.g. JSON) or asking
for other aspects, you could completely change the behavior of this
program. That’s the beauty of programs using LLM prompts. (You could
e.g. also ask for a translation of the content “And please translate the
extracted content to Korean.”).

How are you extracting Markdown from PDF files or images so far? What
experiences did you make with extractions or PDF to Markdown
conversions?

Epilogue: Using Poppler Instead of PyMuPDF Medium member Milan
Agatonovic pointed out correctly that PyMuPDF in the fitz package is not
free for commercial use. Thank you, Milan Agatonovic!

In the Julia solution equivalent to this article (see below), I used
Poppler_jll . We can also use poppler in Python.

```{python}
import os
import tempfile
import subprocess
from pathlib import Path

def convert_pdf_to_images(pdf_path, dpi=300):
    with tempfile.TemporaryDirectory() as tmpdir:
        output_prefix = os.path.join(tmpdir, "page")

        # The Poppler's pdftoppm call
        subprocess.run([
            "pdftoppm",
            "-png",
            "-r", str(dpi),
            pdf_path,
            output_prefix
        ], check=True)

        # Collect all generated PNG images as byte arrays
        png_files = sorted(Path(tmpdir).glob("*.png"))
        images_bytes = [Path(p).read_bytes() for p in png_files]

        return images_bytes
```
